{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for fine mapping based on the PD meta5v2 summary results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan 27, 2020\n",
    "## **Author** - Raph Gibbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set global variables and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up notebook global variables\n",
    "WRKDIR = '$PATH/spd/finemap/meta5v2'\n",
    "AUTOSOMES = [str(x) for x in list(range(1,23))]\n",
    "SEXOMES = ['X','Y']\n",
    "CHROMOSOMES = AUTOSOMES + SEXOMES\n",
    "\n",
    "INDEX_FLANK_SIZE = 1000000\n",
    "# INDEX_FLANK_SIZE = 500000\n",
    "MYUSER = ''\n",
    "GWAS_SIG_PVALUE_THRESHOLD = 5e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import threading\n",
    "import modin.pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import pandas\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# # import math\n",
    "# # import numba\n",
    "# # from umap import UMAP\n",
    "# # import scipy.stats as stats\n",
    "# # import statsmodels.stats.multitest as smm\n",
    "# # import time\n",
    "# from numba import jit\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "#### do the simple thing first, ie using the IPDGC PD meta5v2 summary stats, use more heavily filtered summary stats from CARD (Cornelis pointed me to) path which only contained rsIDs and since had to annotate positions went ahead and used hg38 which makes using TOPMed reference panel since that is in hg38 with postion style variant names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the files local for easier manipulation\n",
    "\n",
    "#path to hold IPDGC meta5v2 original files\n",
    "!mkdir -p {WRKDIR}\n",
    "\n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "\n",
    "summary_results_for_smr = '$PATH/PD/summary_stats/resultsForSmr_filtered.tab.gz'\n",
    "print(f'scp {MYUSER}@helix.nih.gov:{summary_results_for_smr} {WRKDIR}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the summary stats\n",
    "summary_stats_df = pd.read_csv(f'{WRKDIR}/resultsForSmr_filtered.tab.gz', sep='\\t')\n",
    "print(summary_stats_df.shape)\n",
    "summary_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for some reason this files has duplicates, drop them\n",
    "summary_stats_df = summary_stats_df.drop_duplicates(subset=['SNP'])\n",
    "\n",
    "print(summary_stats_df.shape)\n",
    "summary_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### need to map dbSNP rsIDs to reference build positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create limited target function for threading, just run the bash majic\n",
    "def run_cmd_by_majic(the_cmd):\n",
    "    !{the_cmd}\n",
    "\n",
    "#funciton to pull down the UCSC dbSNP files\n",
    "def pull_ucsc_dbsnp_files(version_build_name, chrs_list, out_dir):\n",
    "    snp_bed_url = f'ftp://ftp.ncbi.nih.gov/snp/organisms/{version_build_name}/BED/'\n",
    "    \n",
    "    job_threads = []\n",
    "    for chrom in chrs_list:\n",
    "        this_chr_snp_bed = f'bed_chr_{chrom}.bed.gz'\n",
    "        snp_bed_file = snp_bed_url + this_chr_snp_bed\n",
    "        this_cmd = f'curl --silent -L {snp_bed_file} --output {out_dir}/{this_chr_snp_bed}'\n",
    "#         print(this_cmd)\n",
    "#         !{this_cmd}\n",
    "        this_thread = threading.Thread(target=run_cmd_by_majic, args=(this_cmd,))\n",
    "        job_threads.append(this_thread)\n",
    "        this_thread.start()\n",
    "    \n",
    "    for job_thread in job_threads:\n",
    "        job_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull down hg38 dbSNP b151\n",
    "this_out_dir = f'{WRKDIR}/tools/hg38'\n",
    "!mkdir -p {this_out_dir}\n",
    "\n",
    "# pull_ucsc_dbsnp_files('human_9606_b151_GRCh38p7', AUTOSOMES, this_out_dir)\n",
    "pull_ucsc_dbsnp_files('human_9606_b151_GRCh38p7', CHROMOSOMES, this_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull down GRCh37 dbSNP b151\n",
    "this_out_dir = f'{WRKDIR}/tools/hg19'\n",
    "!mkdir -p {this_out_dir}\n",
    "\n",
    "# pull_ucsc_dbsnp_files('human_9606_b151_GRCh37p13', AUTOSOMES, this_out_dir)\n",
    "pull_ucsc_dbsnp_files('human_9606_b151_GRCh37p13', CHROMOSOMES, this_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to load each chrom's known variants, subset by variants of interest and append\n",
    "#the skiprows argument is has a different effect between modin, dask.dataframe and pandas, \\\n",
    "#have to use default pandas here instead of modin.pandas\n",
    "\n",
    "def find_dbsnp_positions(var_list, chrs_list, build_path):\n",
    "    snp_bed_df = None\n",
    "    for chrom in chrs_list:\n",
    "        print(chrom, end='.')\n",
    "        snp_chr_bed = pandas.read_csv(f'{build_path}/bed_chr_{chrom}.bed.gz', \\\n",
    "                                  header=None, skiprows=1, sep='\\s+')\n",
    "\n",
    "        this_chr = snp_chr_bed.loc[snp_chr_bed[3].isin(var_list),[0, 2, 3]]\n",
    "\n",
    "        snp_bed_df = pandas.concat([snp_bed_df, this_chr])\n",
    "\n",
    "    snp_bed_df.columns = ['chr', 'position', 'id']    \n",
    "    \n",
    "    return snp_bed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#now pull and map ids for index variants\n",
    "build = 'hg38'\n",
    "build_path = f'{WRKDIR}/tools/{build}'\n",
    "\n",
    "#all the variants of interest are proximal to LRRK2 on chr12\n",
    "var_positions_df = find_dbsnp_positions(list(summary_stats_df['SNP']), \\\n",
    "                                             AUTOSOMES, build_path)\n",
    "print(var_positions_df.shape)\n",
    "var_positions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so looks like some number of summary stats rsIDs are not present in dbSNP151\n",
    "missing_rsIDs = set(summary_stats_df['SNP']) - set(var_positions_df['id'])\n",
    "print(len(missing_rsIDs))\n",
    "#33K different in size but only 13K missing lots of dupes?\n",
    "sum_stats_missing_pos_df = summary_stats_df.loc[summary_stats_df['SNP'].\\\n",
    "                                                isin(list(missing_rsIDs))]\n",
    "\n",
    "print(sum_stats_missing_pos_df.shape)\n",
    "print(sum_stats_missing_pos_df['p'].describe())\n",
    "print(sum_stats_missing_pos_df.loc[sum_stats_missing_pos_df['p'] < 1e-08].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dbSNP IDs are no longer valid but had significant results\n",
    "sum_stats_missing_pos_df.loc[sum_stats_missing_pos_df['p'] < 1e-08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summary stats with variants physical position\n",
    "summary_stats_df = summary_stats_df.merge(pd.DataFrame(var_positions_df), \\\n",
    "                                          how='inner', left_on='SNP', right_on='id')\n",
    "\n",
    "summary_stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### interim save point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# go head and do an interim save, for ease of reloading\n",
    "this_h5_file = f'{WRKDIR}/pdmeta_sumstats_hg38.h5'\n",
    "\n",
    "# reload_df = False\n",
    "reload_df = True\n",
    "\n",
    "if reload_df:\n",
    "    print(f'loading {this_h5_file}')\n",
    "    summary_stats_df = pd.read_hdf(this_h5_file)\n",
    "else:\n",
    "    print(f'saving {this_h5_file}')\n",
    "    summary_stats_df.to_hdf(this_h5_file, key='summarystats', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split results into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small function to extract (subset and remove) specified regions summary stats\n",
    "def extract_stats_by_region(source_df, start_bp, stop_bp):\n",
    "    # subset the regions results\n",
    "    result_df = source_df.loc[(source_df['position'] >= start_bp) & \\\n",
    "                              (source_df['position'] <= stop_bp)]\n",
    "    # now remove remove the region from the source \n",
    "    #for some reason, guess referencing, this doesn't work from this function\n",
    "    source_df.drop(result_df.index, inplace=True)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "#function to pull, check, and merge regions when needed\n",
    "def find_regions_for_chrom(stats_df, chrom_str):\n",
    "    #buffer to add of offset and avoid any silly small gaps that might show up\n",
    "    #instead manually setting this buffer make 10% of flank size constant\n",
    "    buffer_size = int(INDEX_FLANK_SIZE * 0.1)\n",
    "\n",
    "    # split results by physical distance, using 1Mb as it what finemap uses in their paper\n",
    "    stats_to_parse = stats_df.loc[stats_df['chr'] == chrom_str].copy()\n",
    "\n",
    "    chroms_regions = []\n",
    "    #get the first row, ie smallest p-value and pull all proximal\n",
    "    while True:\n",
    "        stats_to_parse.sort_values(by='p', inplace=True)\n",
    "        top_row = stats_to_parse.iloc[0]\n",
    "\n",
    "        if top_row['p'] > GWAS_SIG_PVALUE_THRESHOLD:\n",
    "            break\n",
    "\n",
    "        # extract (with deletion of rows) index and flanking variants\n",
    "        this_bp_min = top_row['position'] - INDEX_FLANK_SIZE\n",
    "        this_bp_max = top_row['position'] + INDEX_FLANK_SIZE\n",
    "        this_region = extract_stats_by_region(stats_to_parse, this_bp_min, this_bp_max)\n",
    "\n",
    "        #if no regions for this chrom add, otherwise, see if part of existing region\n",
    "        if not chroms_regions:\n",
    "            chroms_regions.append(this_region)\n",
    "        else:\n",
    "            #check that this next top variant isn't just part of an exsiting region\n",
    "            for list_index, a_region in enumerate(chroms_regions):\n",
    "                a_min_bp = a_region['position'].min()\n",
    "                a_max_bp = a_region['position'].max()\n",
    "\n",
    "                #see if new region overlaps with this enumerated regions\n",
    "                #ie if max end of region greater that min end or region or vice versa\n",
    "                if (this_bp_min > (a_max_bp + buffer_size)) | (this_bp_max < (a_min_bp - buffer_size)):\n",
    "                    #does overlap region at all\n",
    "                    #if this was last regions to check add as new region\n",
    "                    if(list_index == len(chroms_regions)-1):\n",
    "                        chroms_regions.append(this_region)\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                elif (this_bp_min < (a_max_bp + buffer_size)):\n",
    "                    #its a downstream adjacent region\n",
    "                    a_region = pd.concat([a_region, this_region])                \n",
    "                    chroms_regions[list_index] = a_region\n",
    "                    break\n",
    "                elif (this_bp_max > (a_min_bp - buffer_size)):\n",
    "                    #its an upstream adjacent region\n",
    "                    a_region = pd.concat([this_region, a_region])\n",
    "                    chroms_regions[list_index] = a_region    \n",
    "                    break\n",
    "\n",
    "    return chroms_regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# retrieve results by chromosomes\n",
    "#### find chromosomes present in results\n",
    "chroms_arr = summary_stats_df['chr'].unique()\n",
    "\n",
    "job_threads = []\n",
    "regions_by_chrom = {}\n",
    "for chr_str in chroms_arr:\n",
    "    print(chr_str, end='.')\n",
    "    regions_by_chrom[chr_str] = find_regions_for_chrom(summary_stats_df, chr_str)        \n",
    "\n",
    "# need to sort out parallelizing this bit\n",
    "#     this_thread = threading.Thread(target=find_regions_for_chrom, \\\n",
    "#                                    args=(summary_stats_df, chr_str))\n",
    "#     job_threads.append(this_thread)\n",
    "#     this_thread.start()\n",
    "\n",
    "# for job_thread in job_threads:\n",
    "#     job_thread.join()    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write locus regions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write locus regions to file\n",
    "locus_regions_file = f'{WRKDIR}/loci.txt'\n",
    "\n",
    "with open(locus_regions_file,'w') as regions_file:    \n",
    "    #write header\n",
    "    regions_file.write('chrom num_variants index_variant index_p-value start_bp end_bp size_bp\\n')  \n",
    "    for chr_str in chroms_arr:\n",
    "        for this_df in regions_by_chrom[chr_str]:\n",
    "            min_bp = this_df['position'].min()\n",
    "            max_bp = this_df['position'].max()\n",
    "            top_hits = this_df.loc[this_df['p'] == this_df['p'].min()].iloc[0]\n",
    "            top_snp = top_hits['SNP']\n",
    "            smallest_p = top_hits['p']\n",
    "            regions_str = f'{chr_str} {this_df.shape[0]} {top_snp} {smallest_p:.4E} \\\n",
    "{min_bp} {max_bp} {max_bp-min_bp}'\n",
    "\n",
    "            regions_file.write(f'{regions_str}\\n')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy but read that file back in as dataframe\n",
    "regions_df = pd.read_csv(f'{WRKDIR}/loci.txt', sep='\\s+')\n",
    "print(regions_df.shape)\n",
    "regions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity latter, name the loci/region names\n",
    "regions_df['name'] = regions_df['chrom'] + '-' + regions_df['start_bp'].astype('str') + \\\n",
    "'-' + regions_df['end_bp'].astype('str')\n",
    "\n",
    "regions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract regoins from reference panel(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a regions to extract file usable by bcftools\n",
    "regions_file = f'{WRKDIR}/loci_regions.txt'\n",
    "regions_df[['chrom', 'start_bp', 'end_bp']].to_csv(regions_file, header=False, \\\n",
    "                                                   index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC check for single ancestry imputation reference panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sex check mismatches\n",
    "def check_sexcheck(path_prefix):\n",
    "    sexcheck_file = f'{path_prefix}.gender.sexcheck'\n",
    "    sexcheck_df = pd.read_csv(sexcheck_file, sep='\\s+')\n",
    "    #here failed sexcheck does not include unreported sex\n",
    "    sexcheck_problems_df = sexcheck_df.loc[(sexcheck_df['STATUS'] == 'PROBLEM') & \\\n",
    "                                           (sexcheck_df['PEDSEX'] != 0)]\n",
    "    print(f'sex mismatches {sexcheck_problems_df.shape}')\n",
    "    return sexcheck_problems_df\n",
    "\n",
    "#check sample missingness \n",
    "def check_missingness(path_prefix, max_miss=0.05):\n",
    "    smiss_file = f'{path_prefix}.missing.smiss'\n",
    "    #might be plink1 file extenstion\n",
    "    if not os.path.exists(smiss_file):\n",
    "        smiss_file = f'{path_prefix}.missing.imiss'\n",
    "    smiss_df = pd.read_csv(smiss_file, sep='\\s+')\n",
    "    smiss_problems_df = smiss_df.loc[smiss_df['F_MISS'] > max_miss]\n",
    "    print(f'sample missingness problems {smiss_problems_df.shape}')\n",
    "    return smiss_problems_df\n",
    "\n",
    "#het rate problems\n",
    "def check_hetrate(path_prefix, abs_max_rate=0.15):\n",
    "    hetcheck_file = f'{path_prefix}.het.het'\n",
    "    hetcheck_df = pd.read_csv(hetcheck_file, sep='\\s+')\n",
    "    hetcheck_problems_df = hetcheck_df.loc[(hetcheck_df['F'] > abs_max_rate) | \\\n",
    "                                           (hetcheck_df['F'] < -abs_max_rate)]\n",
    "    print(f'sample het rate problems {hetcheck_problems_df.shape}')\n",
    "    return hetcheck_problems_df\n",
    "\n",
    "#sample contamination problems\n",
    "def check_contamination(path_prefix):\n",
    "    contam_file = f'{path_prefix}.contaminated.samples.txt'\n",
    "    #might not exist if didn't process raw data\n",
    "    if os.path.exists(contam_file):\n",
    "        contam_df = pd.read_csv(contam_file, sep='\\s+')\n",
    "        print(f'sample contamination problems {contam_df.shape}')\n",
    "    else:\n",
    "        print('no comtamination file present')\n",
    "        contam_df = pd.DataFrame(data=None, columns=['id'])\n",
    "    return contam_df        \n",
    "\n",
    "#check ancestry outliers\n",
    "def check_ancestry(path_prefix):\n",
    "    pop_outliers_file = f'{path_prefix}.pop.outliers.txt'\n",
    "    pop_outliers = pd.read_csv(pop_outliers_file, sep='\\s+', header=None)\n",
    "    print(f'sample ancestry outliers {pop_outliers.shape}')\n",
    "    return pop_outliers\n",
    "\n",
    "#check sample duplicates (assume no twins)\n",
    "def check_duplicates(path_prefix):\n",
    "    relateds_file = f'{path_prefix}.king.related.kin0'\n",
    "    relateds_df = pd.read_csv(relateds_file, sep='\\s+')\n",
    "    duplicates_df = relateds_df.loc[relateds_df['InfType'] == 'DUP/MZ']\n",
    "    print(f'sample duplicate pairs {duplicates_df.shape}')\n",
    "    return duplicates_df\n",
    "\n",
    "\n",
    "def find_sample_excludes(path_prefix):\n",
    "    sexcheck_problems_df = check_sexcheck(path_prefix)\n",
    "    smiss_problems_df = check_missingness(path_prefix)\n",
    "    hetcheck_problems_df = check_hetrate(path_prefix)\n",
    "    contam_df = check_contamination(path_prefix)\n",
    "    pop_outliers = check_ancestry(path_prefix)\n",
    "    duplicates_df = check_duplicates(path_prefix)\n",
    "    \n",
    "    ref_exclude_set = set(sexcheck_problems_df['IID']) | set(smiss_problems_df['IID']) | \\\n",
    "    set(hetcheck_problems_df['IID']) | set(contam_df['id']) | set(pop_outliers[0])\n",
    "\n",
    "    #note to split duplicate pairs, first break any that currently have QC problems\n",
    "    #then just grab first column\n",
    "    duplicates_df = duplicates_df.loc[~duplicates_df['ID1'].isin(list(ref_exclude_set)) & \\\n",
    "                                     ~duplicates_df['ID1'].isin(list(ref_exclude_set))]\n",
    "    print(f'refreshed duplicate pairs {duplicates_df.shape}')\n",
    "    \n",
    "    ref_exclude_set = ref_exclude_set | set(duplicates_df['ID1'])\n",
    "\n",
    "    print(f'number of samples to exclude from ref panel {len(ref_exclude_set)}')\n",
    "    return ref_exclude_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate an excludes list for the TOPMed set\n",
    "#this of course assumes the files being check in the exclude function exist at \\\n",
    "#this QC path\n",
    "topmed_qc_prefix = '$PATH/datasets/wgs_ref_haplos/qc/topmed.freeze5b'\n",
    "topmed_excludes = find_sample_excludes(topmed_qc_prefix)\n",
    "\n",
    "pd.DataFrame(data=topmed_excludes).to_csv(f'{WRKDIR}/topmed.freeze5b.excludes.txt', \\\n",
    "                                          header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this leaves 16257 reference TOPMed samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset the reference panel vcfs to just PD risk regions and exclude samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset reference panel vcf\n",
    "#these may be slow operations to put into script file for batch processing\n",
    "\n",
    "!mkdir -p {WRKDIR}/reference_panels\n",
    "\n",
    "def subset_loci_from_reference(loci_df, ref_name, in_vcf_name):\n",
    "    script_name = 'vcf_extract'\n",
    "    job_file_name = f'{WRKDIR}/{ref_name}.{script_name}.sh'\n",
    "    with open(job_file_name,'w') as job_file: \n",
    "        for chrom in loci_df['chrom'].unique():\n",
    "            out_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.vcf.gz'\n",
    "            in_vcf = in_vcf_name.format(ref_name, chrom)\n",
    "            extract_cmd = f'bcftools view --regions-file {WRKDIR}/loci_regions.txt \\\n",
    "--output-file {out_vcf} --output-type z --samples-file ^{WRKDIR}/{ref_name}.excludes.txt \\\n",
    "--force-samples --min-ac 5 {in_vcf}'\n",
    "\n",
    "            job_file.write(f'{extract_cmd} &\\n')\n",
    "        job_file.write('wait')\n",
    "        \n",
    "        print('\\n#run these commands at terminal:\\n')\n",
    "        print(f'chmod +x {WRKDIR}/{ref_panel_name}.{script_name}.sh')\n",
    "        print(f'nohup {WRKDIR}/{ref_panel_name}.{script_name}.sh > \\\n",
    "{WRKDIR}/{ref_panel_name}.{script_name}.log &')\n",
    "        \n",
    "#run for ref panel        \n",
    "ref_panel_name = 'topmed.freeze5b'\n",
    "ref_vcf_name = '$PATH/datasets/wgs_ref_haplos/hg38/{}.{}.vcf.gz'\n",
    "subset_loci_from_reference(regions_df, ref_panel_name, ref_vcf_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ensure the variants ID were renamed with alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ensure the variants ID were renamed WITHOUT alleles\n",
    "def re_id_varaints(loci_df, ref_name):\n",
    "    script_name = 'vcf_reid'\n",
    "    job_file_name = f'{WRKDIR}/{ref_name}.{script_name}.sh'\n",
    "    with open(job_file_name,'w') as job_file:    \n",
    "        for chrom in loci_df['chrom'].unique():\n",
    "            out_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.reid.vcf.gz'\n",
    "            in_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.vcf.gz'\n",
    "            annotate_cmd = f'bcftools annotate --output {out_vcf} --output-type z \\\n",
    "--set-id \\'%CHROM\\:%POS\\' {in_vcf}'\n",
    "\n",
    "            job_file.write(f'{annotate_cmd} &\\n')\n",
    "        job_file.write('wait')            \n",
    "\n",
    "        print('\\n#run these commands at terminal:\\n')\n",
    "        print(f'chmod +x {WRKDIR}/{ref_panel_name}.{script_name}.sh')\n",
    "        print(f'nohup {WRKDIR}/{ref_panel_name}.{script_name}.sh > \\\n",
    "{WRKDIR}/{ref_panel_name}.{script_name}.log &')\n",
    "\n",
    "#run for ref panel\n",
    "ref_panel_name = 'topmed.freeze5b'\n",
    "re_id_varaints(regions_df, ref_panel_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make sure dupe variants are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### make sure dupe variants are removed, causes lots of problems with unlabel LD matrices\n",
    "def remove_duplicate_varaints(loci_df, ref_name):\n",
    "    script_name = 'vcf_remove_duplicate_variants'\n",
    "    job_file_name = f'{WRKDIR}/{ref_name}.{script_name}.sh'\n",
    "    with open(job_file_name,'w') as job_file:    \n",
    "        for chrom in loci_df['chrom'].unique():\n",
    "            out_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.reid.nodupes.vcf.gz'\n",
    "            in_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.reid.vcf.gz'\n",
    "            annotate_cmd = f'bcftools norm --rm-dup all --output {out_vcf} --output-type z {in_vcf}'\n",
    "\n",
    "            job_file.write(f'{annotate_cmd} &\\n')\n",
    "        job_file.write('wait')            \n",
    "\n",
    "        print('\\n#run these commands at terminal:\\n')\n",
    "        print(f'chmod +x {WRKDIR}/{ref_panel_name}.{script_name}.sh')\n",
    "        print(f'nohup {WRKDIR}/{ref_panel_name}.{script_name}.sh > \\\n",
    "{WRKDIR}/{ref_panel_name}.{script_name}.log &')\n",
    "\n",
    "#run for ref panel\n",
    "ref_panel_name = 'topmed.freeze5b'\n",
    "remove_duplicate_varaints(regions_df, ref_panel_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert reference panel vcfs to plink1 bfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reference panel vcfs to plink1 bfiles\n",
    "def convert_vcf_plink_bfiles(loci_df, ref_name):\n",
    "    script_name = 'convert_vcf_plink_bfiles'\n",
    "    job_file_name = f'{WRKDIR}/{ref_name}.{script_name}.sh'\n",
    "    with open(job_file_name,'w') as job_file:    \n",
    "        for chrom in loci_df['chrom'].unique():\n",
    "            in_vcf = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.reid.nodupes.vcf.gz'\n",
    "            out_plink = f'{WRKDIR}/reference_panels/{ref_name}.{chrom}.regions.plink'\n",
    "            this_cmd = f'plink --vcf {in_vcf} --make-bed --out {out_plink}'\n",
    "\n",
    "            job_file.write(f'{this_cmd} &\\n')\n",
    "        job_file.write('wait')            \n",
    "\n",
    "        print('\\n#run these commands at terminal:\\n')\n",
    "        !chmod +x {WRKDIR}/{ref_panel_name}.{script_name}.sh\n",
    "        print(f'nohup {WRKDIR}/{ref_panel_name}.{script_name}.sh > \\\n",
    "{WRKDIR}/{ref_panel_name}.{script_name}.log &')\n",
    "\n",
    "#run for ref panel\n",
    "ref_panel_name = 'topmed.freeze5b'\n",
    "convert_vcf_plink_bfiles(regions_df, ref_panel_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run GCTA-COJO\n",
    "\n",
    "GCTA Software tool:\n",
    "Yang et al. (2011) GCTA: a tool for Genome-wide Complex Trait Analysis. Am J Hum Genet. 88(1): 76-82. [PubMed ID: 21167468]\n",
    "\n",
    "https://cnsgenomics.com/software/gcta/#COJO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the tools\n",
    "!curl --silent -L https://cnsgenomics.com/software/gcta/bin/gcta_1.93.0beta.zip \\\n",
    "    --output {WRKDIR}/tools/gcta_1.93.0beta.zip\n",
    "    \n",
    "!unzip {WRKDIR}/tools/gcta_1.93.0beta.zip -d {WRKDIR}/tools/\n",
    "\n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "print(f'sudo cp {WRKDIR}/tools/gcta_1.93.0beta/gcta64 \\\n",
    "/usr/local/bin/gcta64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the gcta-cojo dir exits\n",
    "\n",
    "# create the per locus summary files, the meta results were already formated for smr \\\n",
    "# so same format needed for cojo since both are gcta tools\n",
    "def create_cojo_files(cojo_dir, regions_dict):\n",
    "    for chrom in regions_dict.keys():\n",
    "        for this_df in regions_dict[chrom]:\n",
    "            this_region_results = this_df._to_pandas()\n",
    "            region_start = this_df['position'].min()\n",
    "            region_stop = this_df['position'].max()            \n",
    "            region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "            summary_file = f'{cojo_dir}/input/{region_name}.ma'\n",
    "\n",
    "            #split variant id into postion allele bits\n",
    "            #reid the variants\n",
    "            this_region_results['varid'] = this_region_results['chr'] + ':' + \\\n",
    "this_region_results['position'].astype(str)\n",
    "\n",
    "            this_region_results = this_region_results[['varid', 'A1', 'A2', \\\n",
    "                                                       'freq', 'b', 'se', 'p', 'N']]\n",
    "            this_region_results.to_csv(summary_file, index=False, sep=' ')\n",
    "\n",
    "        \n",
    "#setup the directories    \n",
    "!mkdir -p {WRKDIR}/cojo/input\n",
    "!mkdir -p {WRKDIR}/cojo/output\n",
    "\n",
    "create_cojo_files(f'{WRKDIR}/cojo', regions_by_chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run gcta cojo\n",
    "script_name = 'run_cojo'\n",
    "\n",
    "cojo_minp = GWAS_SIG_PVALUE_THRESHOLD\n",
    "# cojo_minmaf = summary_stats_df['freq'].min()\n",
    "# go with a much smaller maf, here, assuming all summary stats had appropriate cutoff \\\n",
    "# but the that with the LD ref panel there might be edge cases that are just below threshold\n",
    "cojo_minmaf = 0.0001\n",
    "#this will large window won't impact since the summary files are split by the determined regions \\\n",
    "#so just need to make sure full distance of that region is considered in the window \\\n",
    "#but the determined region sizes will likely change the cojo results as some of these will be \\\n",
    "#much larger that 1Mb; 1Mb is not a magical distance threshold anyway\n",
    "cojo_ldwindow = int(regions_df['size_bp'].max()/1000)\n",
    "\n",
    "subjob_cnt = 0\n",
    "with open(f'{WRKDIR}/{script_name}.sh','w') as job_file: \n",
    "    for chrom in regions_by_chrom.keys():\n",
    "        plink_set = f'{WRKDIR}/reference_panels/topmed.freeze5b.{chrom}.regions.plink'\n",
    "        for this_df in regions_by_chrom[chrom]:\n",
    "#             this_region_results = this_df._to_pandas()\n",
    "            this_region_results = this_df.copy()\n",
    "            region_start = this_df['position'].min()\n",
    "            region_stop = this_df['position'].max()            \n",
    "            region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "            summary_file = f'{WRKDIR}/cojo/input/{region_name}.ma'\n",
    "            cojo_out = f'{WRKDIR}/cojo/output/{region_name}'\n",
    "        \n",
    "            job_file.write(f'gcta64 --bfile {plink_set} \\\n",
    "--cojo-file {summary_file} --cojo-slct --cojo-p {cojo_minp} --maf {cojo_minmaf} \\\n",
    "--cojo-wind {cojo_ldwindow} --out {cojo_out} > {cojo_out}.log &\\n')\n",
    "            #run 20 of these at a time\n",
    "            if (subjob_cnt % 20) == 0:\n",
    "                job_file.write('wait\\n')            \n",
    "\n",
    "    job_file.write('wait')\n",
    "              \n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "!chmod +x {WRKDIR}/{script_name}.sh\n",
    "print(f'nohup {WRKDIR}/{script_name}.sh > \\\n",
    "{WRKDIR}/{script_name}.log &') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: for chr7-65584435-67544783 I had to go back and lower the cojo-p to 5e-06 to get a result for this region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the cojo results, and determine causal count per locus\n",
    "cojo_df = None\n",
    "regions_df['causal_cnt'] = 0\n",
    "for row in regions_df.itertuples():\n",
    "    # load regions cojo results\n",
    "    this_df = pd.read_csv(f'{WRKDIR}/cojo/output/{row.name}.jma.cojo', sep='\\s+')\n",
    "    causal_cnt = this_df.shape[0]\n",
    "    regions_df.at[row.Index, 'causal_cnt'] = causal_cnt\n",
    "    cojo_df = pd.concat([cojo_df, this_df])\n",
    "#     print(f'{row.name} count: {causal_cnt}', end=' ')\n",
    "    \n",
    "print(regions_df.head())\n",
    "print(cojo_df.shape)\n",
    "cojo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df['causal_cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df['causal_cnt'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df.loc[regions_df['causal_cnt'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cojo results\n",
    "cojo_df.to_csv(f'{WRKDIR}/cojo/results.jma.cojo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run finemap tool \n",
    "\n",
    "Benner C, Havulinna AS, Salomaa V et al. Refining fine-mapping: effect sizes and regional heritability. bioRxiv 2018:318618.\n",
    "\n",
    "Benner C, Havulinna AS, Järvelin M-R et al. Prospects of Fine-Mapping Trait-Associated Genomic Regions by Using Summary Statistics from Genome-wide Association Studies. Am J Hum Genet 2017, DOI: 10.1016/j.ajhg.2017.08.012.\n",
    "\n",
    "Benner C, Spencer CCA, Havulinna AS et al. FINEMAP: efficient variable selection using summary data from genome-wide association studies. Bioinformatics 2016;32:1493–501.\n",
    "\n",
    "http://www.christianbenner.com/finemap_v1.3.1_x86_64.tgz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the tools\n",
    "!curl --silent -L http://www.christianbenner.com/finemap_v1.3.1_x86_64.tgz \\\n",
    "    --output {WRKDIR}/tools/finemap_v1.3.1_x86_64.tgz\n",
    "    \n",
    "!tar -xf {WRKDIR}/tools/finemap_v1.3.1_x86_64.tgz -C {WRKDIR}/tools/\n",
    "\n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "print(f'sudo cp {WRKDIR}/tools/finemap_v1.3.1_x86_64/finemap_v1.3.1_x86_64 \\\n",
    "/usr/local/bin/finemap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_by_chrom['chr4'][0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reformat meta results for finemap analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finemap uses fileset and control files describing so need to split data \n",
    "# by regions\n",
    "# finemap 1.3.1 with this data always reports max casuals drop back versions for test\n",
    "# in v1.2 z file switched to including summary stats, specified eff_allele as 2nd allele column\n",
    "# in v1.3.1 z file summary stat docs does clearly specify which allele should be effect\n",
    "# the current summary stats file is GCTA format input where 1st allele is effect\n",
    "\n",
    "## ok so kind of looks like 1.3.1 is working exception for the # of causal snps calc, \\\n",
    "## so can specifiy # independent signal from PD meta cojo and still use finemap PIP and cred set(?)\n",
    "\n",
    "## current meta result are in COJO-GCTA summary input format\n",
    "# SNP  - this is the marker name\n",
    "# A1   - this is the effect allele\n",
    "# A2   - the other allele\n",
    "# freq - frequency of the affect allele\n",
    "# b    - beta, coeff, effect for A1\n",
    "# se   - overall standard error for effect size estimate\n",
    "# p    - meta-analysis p-value\n",
    "# N    - number of sample included in test\n",
    "\n",
    "def create_finemap_files(fm_dir, regions_dict):\n",
    "    for chrom in regions_by_chrom.keys():\n",
    "        for this_df in regions_by_chrom[chrom]:\n",
    "            this_region_results = this_df._to_pandas()\n",
    "            region_start = this_df['position'].min()\n",
    "            region_stop = this_df['position'].max()            \n",
    "            region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "#             print(region_name, end=' ')\n",
    "            #set job file names\n",
    "            cntrl_file_name = f'{fm_dir}/input/{region_name}.control'\n",
    "            z_file = f'{fm_dir}/input/{region_name}.z'\n",
    "            ld_file = f'{fm_dir}/input/{region_name}.ld'\n",
    "            snp_file = f'{fm_dir}/output/{region_name}.snp'\n",
    "            cfg_file = f'{fm_dir}/output/{region_name}.config'\n",
    "            cred_file = f'{fm_dir}/output/{region_name}.cred'\n",
    "            log_file = f'{fm_dir}/output/{region_name}.log'\n",
    "\n",
    "\n",
    "            #split variant id into postion allele bits\n",
    "            #reid the variants\n",
    "            this_region_results['varid'] = this_region_results['chr'] + ':' + \\\n",
    "this_region_results['position'].astype(str)\n",
    "\n",
    "            #calculate the maf based on freq1\n",
    "            this_region_results['maf'] = np.where(this_region_results['freq'] < 0.50, \\\n",
    "                                                  this_region_results['freq'], \\\n",
    "                                                  1-this_region_results['freq']).round(4)\n",
    "            \n",
    "            n_samples = int(this_region_results['N'].mean())\n",
    "\n",
    "            #gcta expects effect allele first, snptest/finemap expect second\n",
    "            #this is for v1.3.1\n",
    "            this_region_results = this_region_results[['varid', 'chr', 'position', \\\n",
    "                                                       'A2', 'A1', 'maf', 'b', 'se']]\n",
    "            this_region_results.columns = ['rsid', 'chromosome', 'position', 'allele1', \\\n",
    "                                           'allele2', 'maf', 'beta', 'se']\n",
    "\n",
    "            this_region_results.to_csv(z_file, index=False, sep=' ')\n",
    "\n",
    "            #this is for v1.3.1\n",
    "            contro_file_header = 'z;ld;snp;config;cred;n_samples;log'\n",
    "\n",
    "            with open(cntrl_file_name,'w') as cntrl_file:\n",
    "                cntrl_file.write(f'{contro_file_header}\\n')\n",
    "                #this is for v1.3.1\n",
    "                cntrl_file.write(f'{z_file};{ld_file};{snp_file};{cfg_file};\\\n",
    "{cred_file};{n_samples};{log_file}\\n')        \n",
    "        \n",
    "#setup the directories    \n",
    "!mkdir -p {WRKDIR}/finemap/input\n",
    "!mkdir -p {WRKDIR}/finemap/output\n",
    "\n",
    "create_finemap_files(f'{WRKDIR}/finemap', regions_by_chrom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trim z files so that only variants that are also in the reference are considered\n",
    "unfortunately this means we are going to loose variants, not great for fine mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### trim z files so that only variants that are also in the reference are considered\n",
    "def check_finemap_zfiles(fm_dir, regions_dict, ref_dif):\n",
    "    missing_counts = []\n",
    "    \n",
    "    for chrom in regions_dict.keys():\n",
    "        #go ahead and load the bim df since we are doing this per chrom not per region on chrom\n",
    "        try:\n",
    "            bim_file = f'{ref_dif}/topmed.freeze5b.{chrom}.regions.plink.bim'        \n",
    "            bim_df = pd.read_csv(bim_file, header=None, sep='\\s+')\n",
    "            bim_set = set(bim_df[1]) #col 1 is varaint names\n",
    "        except IOError:\n",
    "            print(f'{chrom} doesn\\'t have a file')\n",
    "            continue\n",
    "            \n",
    "        for this_df in regions_dict[chrom]:\n",
    "            this_region_results = this_df._to_pandas()\n",
    "            region_start = this_df['position'].min()\n",
    "            region_stop = this_df['position'].max()            \n",
    "            region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "            z_file = f'{fm_dir}/input/{region_name}.z'\n",
    "\n",
    "            #load the dataframes\n",
    "            z_df = pd.read_csv(z_file, sep='\\s+')\n",
    "\n",
    "            lost_var_count = len(set(z_df['rsid']) - bim_set)\n",
    "            missing_counts.append(lost_var_count)\n",
    "            \n",
    "            print(f'{region_name} lost {lost_var_count}', end=' ')\n",
    "            \n",
    "            #now remove these missings and save new z file\n",
    "            z_df = z_df.loc[z_df['rsid'].isin(bim_set)]\n",
    "            z_df.to_csv(z_file, header=True, index=False, sep=' ')\n",
    "\n",
    "    print(f'\\nmean loss per region {np.mean(missing_counts)}')\n",
    "\n",
    "#run the function\n",
    "check_finemap_zfiles(f'{WRKDIR}/finemap', regions_by_chrom, f'{WRKDIR}/reference_panels')            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the ld files named in the created control files\n",
    "fm_dir = f'{WRKDIR}/finemap'\n",
    "\n",
    "job_name = 'generate.ld_matrices'\n",
    "job_file_name = f'{fm_dir}/{job_name}.sh'\n",
    "with open(job_file_name,'w') as job_file:    \n",
    "    for chrom in regions_by_chrom.keys():\n",
    "        for this_df in regions_by_chrom[chrom]:\n",
    "            this_region_results = this_df._to_pandas()\n",
    "            region_start = this_df['position'].min()\n",
    "            region_stop = this_df['position'].max()            \n",
    "            region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "\n",
    "            ld_file = f'{fm_dir}/input/{region_name}'\n",
    "            z_file = f'{fm_dir}/input/{region_name}.z'\n",
    "            ref_plink = f'{WRKDIR}/reference_panels/topmed.freeze5b.{chrom}\\\n",
    ".regions.plink'\n",
    "\n",
    "            job_file.write(f'plink --bfile {ref_plink} --r2 square \\\n",
    "--out {ld_file} --extract {z_file}\\n')\n",
    "            job_file.write(f'sed -i s\\\"/\\\\t/ /\\\"g {ld_file}.ld\\n')\n",
    "        \n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "!chmod +x {job_file_name}\n",
    "print(f'nohup {job_file_name} > {fm_dir}/{job_name}.log &')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create script file for running the finemap jobs\n",
    "script_name = 'run_finemap'\n",
    "\n",
    "subjob_cnt = 0\n",
    "with open(f'{WRKDIR}/{script_name}.sh','w') as job_file: \n",
    "    for locus_row in regions_df.itertuples():\n",
    "        subjob_cnt += 1\n",
    "\n",
    "        cntrl_file_name = f'{fm_dir}/input/{locus_row.name}.control'\n",
    "\n",
    "        job_file.write(f'finemap --sss --in-files {cntrl_file_name} \\\n",
    "--log --n-causal-snps {locus_row.causal_cnt} &\\n')\n",
    "        #run 20 of these at a time\n",
    "        if (subjob_cnt % 20) == 0:\n",
    "            job_file.write('wait\\n')\n",
    "        \n",
    "    job_file.write('wait')\n",
    "              \n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "!chmod +x {WRKDIR}/{script_name}.sh\n",
    "print(f'nohup {WRKDIR}/{script_name}.sh > \\\n",
    "{WRKDIR}/{script_name}.log &') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check expected results are present\n",
    "print(f'should be {regions_df.shape[0]} of each of these file types')\n",
    "\n",
    "!ls {fm_dir}/output/*.snp | wc -l\n",
    "!ls {fm_dir}/output/*.config | wc -l\n",
    "!ls {fm_dir}/output/*.cred | wc -l\n",
    "!ls {fm_dir}/output/*.log_sss | wc -l\n",
    "\n",
    "!less {WRKDIR}/{script_name}.log | grep Error\n",
    "\n",
    "print('\\n#run these commands at terminal:\\n')\n",
    "print(f'ls -lhS {fm_dir}/output/*.snp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse finemap results to fill in blocks table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse finemap results to fill in blocks table and merge original summary status with finemap PIPs\n",
    "fm_sumstats_df = None\n",
    "regions_df['tested_vars'] = 0\n",
    "regions_df['pip_vars'] = 0\n",
    "regions_df['h2'] = 0\n",
    "for chrom in regions_by_chrom.keys():\n",
    "    print(chrom, end=' ')\n",
    "    for this_df in regions_by_chrom[chrom]:\n",
    "        this_region_results = this_df._to_pandas()\n",
    "        region_start = this_region_results['position'].min()\n",
    "        region_stop = this_region_results['position'].max()            \n",
    "        region_name = f'{chrom}-{region_start}-{region_stop}'\n",
    "        #finemap output files to read\n",
    "        snp_file = f'{fm_dir}/output/{region_name}.snp'\n",
    "        cfg_file = f'{fm_dir}/output/{region_name}.config'\n",
    "        cred_file = f'{fm_dir}/output/{region_name}.cred'\n",
    "    \n",
    "        if not os.path.exists(cred_file):\n",
    "            print(cred_file)\n",
    "            continue\n",
    "    \n",
    "        pip_df = pd.read_csv(snp_file, sep='\\s+')\n",
    "\n",
    "        region_stats_df = this_region_results.loc[this_region_results['position'].isin(pip_df['position'])]\n",
    "        region_stats_df = region_stats_df.merge(pip_df._to_pandas(), how='inner', \\\n",
    "                                                left_on='position', right_on='position')\n",
    "\n",
    "        fm_sumstats_df = pd.concat([fm_sumstats_df, region_stats_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the full pd_regions fine mapping results\n",
    "fm_sumstats_df.to_csv(f'{fm_dir}/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fm_sumstats_df.shape)\n",
    "print(fm_sumstats_df.columns)\n",
    "fm_sumstats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update and save the loci summary\n",
    "for locus in regions_df.itertuples():\n",
    "    #finemap output files to read\n",
    "    snp_file = f'{fm_dir}/output/{locus.name}.snp'\n",
    "    cfg_file = f'{fm_dir}/output/{locus.name}.config'\n",
    "\n",
    "    pip_df = pd.read_csv(snp_file, sep='\\s+')\n",
    "    cfg_df = pd.read_csv(cfg_file, sep='\\s+')\n",
    "    regions_df.at[locus.Index, 'tested_vars'] = pip_df.shape[0]\n",
    "    regions_df.at[locus.Index, 'pip_vars'] = pip_df[pip_df['prob'] > 0.01].shape[0]  \n",
    "    regions_df.at[locus.Index, 'h2'] = cfg_df['h2'].max()\n",
    "    \n",
    "#now save updated locus summary\n",
    "regions_df.to_csv(f'{WRKDIR}/loci_fm_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regions_df['size_bp'].sum())\n",
    "print(regions_df['causal_cnt'].sum())\n",
    "print(regions_df['tested_vars'].sum())\n",
    "print(regions_df['pip_vars'].sum())\n",
    "print(regions_df['h2'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regions_df['size_bp'].describe())\n",
    "print(regions_df['causal_cnt'].describe())\n",
    "print(regions_df['tested_vars'].describe())\n",
    "print(regions_df['pip_vars'].describe())\n",
    "print(regions_df['h2'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_df['pip_vars'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot example local manhattans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if skipped down here just read in results from above\n",
    "fm_sumstats_df = pd.read_csv(f'{WRKDIR}/finemap/results.csv')\n",
    "print(fm_sumstats_df.shape)\n",
    "fm_sumstats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local manhattan plot with p-value and PIP\n",
    "def lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop):\n",
    "    # fm_sumstats_dd = dd.from_pandas(fm_sumstats_df,8)\n",
    "    this_region_results = fm_sumstats_df.loc[(fm_sumstats_df['chromosome'] == chrom) & \\\n",
    "                                             (fm_sumstats_df['position'] > block_start) & \\\n",
    "                                             (fm_sumstats_df['position'] < block_stop)]\n",
    "\n",
    "    print(this_region_results.shape)\n",
    "\n",
    "    plt.figure(figsize=(18,18))\n",
    "    sns.set_style('dark')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(12, 8)\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    sns.scatterplot(x=this_region_results['position']/1000, size=abs(this_region_results['z']), \\\n",
    "                    y=np.log10(this_region_results['p'])*-1, color='grey', ax=ax, legend=False)\n",
    "    \n",
    "#     y_level = (np.log10(this_region_results['p'])*-1)/2\n",
    "#     plt.plot([gene_start/1000, gene_stop/1000], [y_level, y_level], linewidth=2)\n",
    "#     plt.text(gene_start/10002, gene_stop/1000, gene_name)\n",
    "\n",
    "    sns.scatterplot(x=this_region_results['position']/1000 , size=abs(this_region_results['z']), \\\n",
    "                    y=this_region_results['prob'], color='red', ax=ax2, legend=False)\n",
    "    # ax2.set_ylim(0,this_region_results['log10bf'].max()+1)\n",
    "    ax2.set_ylim(0,1.2)\n",
    "\n",
    "    ax.set(xlabel='chromosome position (kb)', ylabel='-log10(p-value)')\n",
    "    ax2.set(xlabel='chromosome position (kb)', ylabel='Posterior Inclusion Probability')\n",
    "\n",
    "    plt.title(f'PD locus {chrom} near {gene_name}',fontsize='large') \n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0,prop={'size': 8})\n",
    "\n",
    "    plot_out_file_name = f'{WRKDIR}/finemap/images/{chrom}.{gene_name}.local_man.png'\n",
    "    plt.savefig(plot_out_file_name,format='png',dpi=600,bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p '{WRKDIR}/finemap/images'\n",
    "\n",
    "#INPP5F\n",
    "chrom = 'chr10'\n",
    "block_start = 118777099\n",
    "block_stop = 120776542\n",
    "\n",
    "gene_name = 'INPP5F'\n",
    "gene_start = 119726050\n",
    "gene_stop = 119829147\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BST1\n",
    "chrom = 'chr4'\n",
    "block_start = 14735725\n",
    "block_stop = 16735661\n",
    "\n",
    "gene_name = 'BST1'\n",
    "gene_start = 15703065\n",
    "gene_stop = 15732787\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUCKS1\n",
    "chrom = 'chr1'\n",
    "block_start = 204754670\n",
    "block_stop = 206754269\n",
    "\n",
    "gene_name = 'NUCKS1'\n",
    "gene_start = 205712822\n",
    "gene_stop = 205750182\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPNMB\n",
    "chrom = 'chr7'\n",
    "block_start = 22260625\n",
    "block_stop = 24260398\n",
    "\n",
    "gene_name = 'GPNMB'\n",
    "gene_start = 23246746\n",
    "gene_stop = 23275096\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMEM175\n",
    "chrom = 'chr4'\n",
    "block_start = 65411\n",
    "block_stop = 1956747\n",
    "\n",
    "gene_name = 'TMEM175'\n",
    "gene_start = 932460\n",
    "gene_stop = 958656\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SNCA\n",
    "chrom = 'chr4'\n",
    "block_start = 88705240\n",
    "block_stop = 90704927\n",
    "\n",
    "gene_name = 'SNCA'\n",
    "gene_start = 89724099\n",
    "gene_stop = 89838315\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTSB\n",
    "chrom = 'chr8'\n",
    "block_start = 10855435\n",
    "block_stop = 12854684\n",
    "\n",
    "gene_name = 'CTSB'\n",
    "gene_start = 11842524\n",
    "gene_stop = 11868087\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNTN1\n",
    "chrom = 'chr12'\n",
    "block_start = 39341546\t\n",
    "block_stop = 42901508\n",
    "\n",
    "gene_name = 'LRRK2-CNTN1'\n",
    "gene_start = 40692442\n",
    "gene_stop = 41072412\n",
    "\n",
    "lcl_manhattan_plot(chrom, block_start, block_stop, gene_name, gene_start, gene_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare paper cojo results with these cojo results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PD meta5v2 Table S2 results\n",
    "table_s2_df = pd.read_csv(f'{WRKDIR}/table_s2.csv')\n",
    "print(f'Table S2: {table_s2_df.shape}')\n",
    "\n",
    "# reload cojo results\n",
    "cojo_df = pd.read_csv(f'{WRKDIR}/cojo/results.jma.cojo.csv')\n",
    "print(f'cojo: {cojo_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#now pull and map ids for index variants\n",
    "build = 'hg38'\n",
    "build_path = f'{WRKDIR}/tools/{build}'\n",
    "\n",
    "#all the variants of interest are proximal to LRRK2 on chr12\n",
    "var_positions_df = find_dbsnp_positions(list(table_s2_df['SNP']), \\\n",
    "                                             AUTOSOMES, build_path)\n",
    "print(var_positions_df.shape)\n",
    "var_positions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so looks like some number of summary stats rsIDs are not present in dbSNP151\n",
    "missing_rsIDs = set(table_s2_df['SNP']) - set(var_positions_df['id'])\n",
    "print(len(missing_rsIDs))\n",
    "#33K different in size but only 13K missing lots of dupes?\n",
    "index_var_stats_missing_pos_df = table_s2_df.loc[table_s2_df['SNP'].\\\n",
    "                                                isin(list(missing_rsIDs))]\n",
    "\n",
    "print(index_var_stats_missing_pos_df.shape)\n",
    "print(index_var_stats_missing_pos_df['P'].describe())\n",
    "print(index_var_stats_missing_pos_df.loc[table_s2_df['P'] < 1e-08].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dbSNP IDs are no longer valid but had significant results\n",
    "index_var_stats_missing_pos_df.loc[index_var_stats_missing_pos_df['P'] < 1e-08]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge summary stats with variants physical position\n",
    "table_s2_df = table_s2_df.merge(pd.DataFrame(var_positions_df), \\\n",
    "                                how='inner', left_on='SNP', right_on='id')\n",
    "\n",
    "table_s2_df['id'] = table_s2_df['chr'] + ':' + table_s2_df['position'].astype('str')\n",
    "table_s2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(cojo_df['SNP']) & set(table_s2_df['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = table_s2_df._to_pandas()\n",
    "# display(temp_df.loc[(~temp_df['id'].isin(cojo_df['SNP'])) & \\\n",
    "#                     (temp_df['chr'] == 'chr12')])\n",
    "\n",
    "display(table_s2_df.loc[(~table_s2_df['id'].isin(cojo_df['SNP'])) & \\\n",
    "                        (table_s2_df['chr'] == 'chr12')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many of the paper cojos are PIPs here\n",
    "fm_sumstats_df.shape\n",
    "pips_df = fm_sumstats_df.loc[fm_sumstats_df['prob'] > 0.01]\n",
    "print(pips_df.shape)\n",
    "\n",
    "print(len(set(table_s2_df['SNP']) & set(pips_df['SNP'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many of the this analysis cojos are PIPs here\n",
    "print(len(set(cojo_df['SNP']) & set(pips_df['rsid'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many of the these are all three\n",
    "print(len((set(table_s2_df['SNP']) & set(cojo_df['SNP'])) & set(pips_df['rsid'])))\n",
    "#huh wasn't what I was necessarily expecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should take a look to see how cojo matches line up with one-signal loci\n",
    "#but given the zero above probably again not what I'm guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
